

![i1](https://user-images.githubusercontent.com/117050213/205548116-1e449d16-574f-4a21-8b97-7350506b2992.png)
![i3](https://user-images.githubusercontent.com/117050213/205548119-5701fde0-ce47-4487-aa38-287b375e4e87.png)
![i2](https://user-images.githubusercontent.com/117050213/205548120-1b2d9fae-bdc7-48fd-b148-6444cc781a8d.png)
![i4](https://user-images.githubusercontent.com/117050213/205548121-385094dc-89ae-42d6-b7c9-aa028aad96cd.png)
![i5](https://user-images.githubusercontent.com/117050213/205548249-6233b364-41ac-4123-bc99-555564cbf6b6.png)

For the 4th milestone in the semantic segmentation project, the goal was to perform model compression. This can optimize models to allow on computers with low memory and GPU. To perform model compression, there are two methods known as pruning and quantization. Model pruning is a method that removes weight from a model by zeroing out any parameters that are found to be redundant. Quantization compresses models by reducing the number of bits that represent weights and activation to reduce run time and memory without significant decrease in the model’s accuracy. Through quantization, a model’s features can be compressed to sizes as small as 8 bits while using Tensorflows
The loss came to a consistent 89%, the accuracy was 90%, and the mean IoU was 0.55. Compared to our unpruned model, our loss only rose by around 1% and the accuracy which was between 89-93% got up to 90%. This shows the inconsequentiality of the pruned parameters as removing them made very small changes to our loss and accuracy. Although our mean IoU went from 0.59 to 0.55, it staying above .50 means our model is still accurate enough to overlay blocks correctly so this drop is not too significant.
Then there was quantization to compress our model into something operable on computers without access to high RAM. After this conversion, our model was small enough for use on more common computers. In the end, the model became a better fit for the common computer, while still maintaining its performance.
